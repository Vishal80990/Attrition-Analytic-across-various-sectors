{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8bbb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install fosforml numpy pandas matplotlib scikit-learn seaborn python-dateutil\n",
    "!pip uninstall urllib3 -y\n",
    "!pip install urllib3==1.26.15\n",
    "!pip install fosforml \n",
    "!pip install fosforio\n",
    "!pip install -U cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc44edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn scipy xgboost pandas dice-ml tabulate numpy scikit-learn pandas-profiling plotly matplotlib scipy statsmodels seaborn pydantic-settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b12a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from scipy.stats.mstats import winsorize\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from fosforml import *\n",
    "from fosforml.constants import MLModelFlavours\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574798e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml.model_manager.snowflakesession import get_session\n",
    "\n",
    "my_session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ff62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 'ATTRITION_TABLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e748d-8abc-410c-bf48-be2206e0707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = my_session.sql(\"select * from {}\".format(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ef637-cba9-4cf7-a388-9f6ff4127fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = sf_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927662fa-ee34-4e48-b27a-2250be087ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_df = pandas_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Original_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Original_df.drop([\"USER_ID\", \"EMPLOYEE_ID\", \"JOB_STARTDATE\", \"JOB_ENDDATE\", \"SCHOOL_ENDDATE\",\"CHURN_VALUE\",\"PEOPLE_JOINED_BEFORE_AND_LEFT_IN_THIS_MONTH\",\"PEOPLE_JOINED_AND_NEVER_LEFT\",\"POPULATION\",\"CHURN_OTHER\",\"CHURN_F\",\"SUM_OF_TENURE\",\"SUM_OF_AGE\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f28381",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"MAPPED_ROLE\",\"GENDER\", \"ETHNICITY\",\"ORGANIZATION_TYPE\", \"ORGANIZATION_OWNERSHIP\",\"COMPANY_NAME\",\"CITY\",\"STATE\",\"DISTANCE\",\"COUNTRY\",\"DEGREE_CLEAN\",\n",
    "                       \"BUSINESS_TRAVEL\",\"ENVIRONMENT_SATISFACTION\",\"JOB_SATISFACTION\",\"MARITAL_STATUS\",\"OVER_TIME\",\"PERFORMANCE_RATING\",\"RELATIONSHIP_SATISFACTION\",\"WORK_LIFE_BALANCE\"]\n",
    "NUMERICAL_COLUMNS = [\"SALARY\", \"SENIORITY\", \"TENURE_MONTHS\", \"MONTHS_AFTER_COLLEGE\", \"BIRTH_YEAR\",\"AGE\",\"OVERTIME_HOURS\",\"PERCENTAGE_SALARY_HIKE\"]\n",
    "LABEL_COLUMNS = [\"CHURN\"]\n",
    "DROPPED_COLUMNS = [\"USER_ID\", \"EMPLOYEE_ID\", \"JOB_STARTDATE\", \"JOB_ENDDATE\", \"SCHOOL_ENDDATE\",\"CHURN_VALUE\",\"PEOPLE_JOINED_BEFORE_AND_LEFT_IN_THIS_MONTH\",\"PEOPLE_JOINED_AND_NEVER_LEFT\",\"POPULATION\",\"CHURN_OTHER\",\"CHURN_F\",\"SUM_OF_TENURE\",\"SUM_OF_AGE\"]\n",
    "OUTPUT_COLUMNS = [\"PREDICTION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter feature columns\n",
    "feature_columns = CATEGORICAL_COLUMNS + NUMERICAL_COLUMNS\n",
    "feature_columns = [col for col in feature_columns if col in Original_df.columns]\n",
    "LABEL_COLUMNS = [col for col in LABEL_COLUMNS if col in Original_df.columns]\n",
    " \n",
    "# Split data into features and labels\n",
    "X = Original_df[feature_columns + DROPPED_COLUMNS]\n",
    "y = Original_df[LABEL_COLUMNS].values.ravel()  # Flatten to 1D array for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7786299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c79dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    " \n",
    "# Define transformers\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='missing'),\n",
    "    OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    ")\n",
    " \n",
    "numerical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    MinMaxScaler(clip=True)\n",
    ")\n",
    " \n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, CATEGORICAL_COLUMNS),\n",
    "        ('num', numerical_transformer, NUMERICAL_COLUMNS)\n",
    "    ]\n",
    ")\n",
    " \n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "result = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bc983",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f24f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "filename = \"HR_Attrition.joblib\"\n",
    "dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c20f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "y_prob = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml import *\n",
    "\n",
    "from fosforml.constants import MLModelFlavours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scoring_func\n",
    "def score(model, request):\n",
    "    payload_dict = request.json[\"payload\"]\n",
    "    data_json = eval(payload)\n",
    "    data = pd.DataFrame([data_json])\n",
    "    prediction = str(model.predict(data)[0])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b764b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd \n",
    "payload = str(X_test.iloc[123].to_dict())\n",
    "req = requests.Request()\n",
    "req.json = {\"payload\": payload}\n",
    "\n",
    "print(score(pipeline, req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5169f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "req.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## registering the model.\n",
    "tmp = register_model(pipeline, \n",
    "               score, \n",
    "               name=\"HR_ATTRITION_ML_MODEL\", \n",
    "               description=\"Analyzing_HR_Attrition_trained_using _ml\",\n",
    "               flavour=MLModelFlavours.sklearn,\n",
    "               model_type=\"classification\",\n",
    "               init_script=\"\\\\n pip install fosforml \\\\n pip install seaborn \\\\n pip install snowflake-connector-python[pandas] \\\\n pip install joblib==1.3.2 scikit-learn=1.3.2\",\n",
    "               y_true=y_test,\n",
    "               y_pred=y_pred, \n",
    "               features=X_train.columns,\n",
    "               labels=[0,1],\n",
    "               input_type=\"json\", \n",
    "               explain_ai=True,\n",
    "               prob=y_prob,\n",
    "               x_train=X_train, \n",
    "               x_test=X_test, \n",
    "               y_train=y_train,\n",
    "               y_test=y_test,\n",
    "               feature_names=X_train.columns.tolist(),\n",
    "               original_features=X_train.columns.tolist(),\n",
    "               feature_ids=X_train.columns,\n",
    "               target_names=['NOT LEFT','LEFT'],\n",
    "               kyd=True, kyd_score = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff88e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload  = {\"payload\": X_test.iloc[0].to_dict()}\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ef3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.predict(X_test)\n",
    "result_prob = pipeline.predict_proba(X_test)\n",
    "pred_df = X_test.copy()\n",
    "result = result\n",
    "result_prob = result_prob\n",
    "pred_df[\"PREDICTION\"] = result\n",
    "pred_df[\"PROB\"] = result_prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1df31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, log_loss, roc_auc_score\n",
    " \n",
    "# Check lengths\n",
    "print(\"Length of y_test:\", len(y_test))\n",
    "print(\"Length of y_pred:\", len(y_pred))\n",
    " \n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy = sum(y_test == y_pred) / len(y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    " \n",
    "# Calculate additional metrics\n",
    "log_loss_value = log_loss(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob[:, 1])  # Assuming class 1 is the positive class\n",
    " \n",
    "print(\"Log Loss:\", log_loss_value)\n",
    "print(\"ROC AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c775c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, pred_df[\"PROB\"])\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, pred_df[\"PROB\"])\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Radnomforest')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f661cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, pred_df[\"PROB\"])\n",
    "plt.plot(lr_recall, lr_precision, marker='.', label='Randomforest')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    " \n",
    "#Plot the confusion matrix.\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt='g',\n",
    "            xticklabels=['Not Churn','Churn'],\n",
    "            yticklabels=['Not CHurn','Churn'])\n",
    "plt.ylabel('Prediction',fontsize=13)\n",
    "plt.xlabel('Actual',fontsize=13)\n",
    "plt.title('Confusion Matrix',fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_data = pd.DataFrame({\"ns_probs\":ns_probs,\n",
    "                            \"y_test\":y_test,\n",
    "                            \"y_pred\":pred_df[\"PREDICTION\"],\n",
    "                            \"act_probs\":pred_df[\"PROB\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ca252",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_data.to_csv(\"scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5599fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define or use the prediction function\n",
    "def model_prediction_score_func(dataframe):\n",
    "    # Ensure 'dataframe' has the correct features required by the model\n",
    "    return pipeline.predict(dataframe)  # Use your trained pipeline/model here\n",
    " \n",
    "# Assuming df is your input DataFrame with the necessary features\n",
    "Original_df['Model_Output'] = model_prediction_score_func(Original_df)\n",
    " \n",
    "# If you have a DataFrame with test data and you want to merge predictions\n",
    "# Assuming X_test is the DataFrame for which predictions are made\n",
    "#X_test_with_predictions = X_test.copy()\n",
    "#X_test_with_predictions['Model_Output'] = model_prediction_score_func(X_test)\n",
    " \n",
    "# Display the first few rows to verify\n",
    "#print(X_test_with_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_df.to_csv(\"Attrition_Data_predictive.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "if 'Model_Output' in Original_df:\n",
    "    Original_df['Model_Output_Values'] = Original_df['Model_Output'].astype(bool)\n",
    "print(Original_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = my_session.createDataFrame(Original_df)\n",
    "sf_df.write.mode(\"overwrite\").save_as_table(\"Attrition_Data_predictive\")\n",
    "my_session.table(\"Attrition_Data_predictive\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e3bf39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
